{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de Team_Microsoft.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EhAq-1bHL7f"
      },
      "source": [
        "# Team Microsoft\n",
        "\n",
        "## 1: Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqybCM22GNoK"
      },
      "source": [
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import sklearn\n",
        "import csv,json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import collections  as mc\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stpw\n",
        "\n",
        "#sns.set_style(\"white\")\n",
        "#%load_ext autoreload\n",
        "#%autoreload 2\n",
        "#from sklearn import datasets\n",
        "\n",
        "\n",
        "#%load_ext autoreload\n",
        "#%autoreload 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdfvQrfuHugA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "ba496998-feb5-4a54-91c3-a59b62d5a1d0"
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/KVVisDon/Data-Mining-Project/main/data/training_data.csv\")\n",
        "\n",
        "\n",
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/KVVisDon/Data-Mining-Project/main/data/test_data.csv\")\n",
        "\n",
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5988</td>\n",
              "      <td>hazardous</td>\n",
              "      <td>USA</td>\n",
              "      <td>#Taiwan Grace: expect that large rocks trees m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6328</td>\n",
              "      <td>hostage</td>\n",
              "      <td>Australia</td>\n",
              "      <td>New ISIS Video: ISIS Threatens to Behead Croat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6466</th>\n",
              "      <td>4377</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>ARGENTINA</td>\n",
              "      <td>#Earthquake #Sismo M 1.9 - 15km E of Anchorage...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6467</th>\n",
              "      <td>3408</td>\n",
              "      <td>derail</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@EmiiliexIrwin Totally agree.She is 23 and kno...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6468</th>\n",
              "      <td>9794</td>\n",
              "      <td>trapped</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hollywood Movie About Trapped Miners Released ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6469</th>\n",
              "      <td>10344</td>\n",
              "      <td>weapons</td>\n",
              "      <td>Beirut/Toronto</td>\n",
              "      <td>Friendly reminder that the only country to eve...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6470</th>\n",
              "      <td>1779</td>\n",
              "      <td>buildings%20on%20fire</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Buildings are on fire and they have time for a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6471 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ... target\n",
              "0      3738  ...      0\n",
              "1       853  ...      0\n",
              "2     10540  ...      1\n",
              "3      5988  ...      1\n",
              "4      6328  ...      1\n",
              "...     ...  ...    ...\n",
              "6466   4377  ...      1\n",
              "6467   3408  ...      0\n",
              "6468   9794  ...      1\n",
              "6469  10344  ...      1\n",
              "6470   1779  ...      1\n",
              "\n",
              "[6471 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgJQp-GcLSFu"
      },
      "source": [
        "## 1.2: Base rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-NpB7-yKVNl",
        "outputId": "f55000c6-5358-42a6-92a7-1cabdd5c5f8d"
      },
      "source": [
        "br = df.target.value_counts().max()/df.target.value_counts().sum()\n",
        "number_per_class = df.target.value_counts()\n",
        "print(number_per_class)\n",
        "print(\"Base rate is\",str(round(br,2)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    3701\n",
            "1    2770\n",
            "Name: target, dtype: int64\n",
            "Base rate is 0.57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6PgvxmCMBVj"
      },
      "source": [
        "# 2: Cleaning\n",
        "###Basic : we defined a preprocessing and a tokenzation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "yoxNFpTuMMbN",
        "outputId": "95cddd91-3946-4b48-ce26-437855a5884e"
      },
      "source": [
        "sp = spacy.load('en_core_web_sm')\n",
        "punctuations = string.punctuation\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "def spacy_token(message):\n",
        "    \n",
        "  mytokens = sp(message)\n",
        "  # Lemmatize each token and convert each token into lowercase\n",
        "  mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "  # Remove stop words and punctuation\n",
        "  mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "  # Remove hashtags, @\n",
        "  mytokens = [ word.replace('@','').replace('#', '') for word in mytokens ]\n",
        "\n",
        "  mytokens = [ word.replace('rt @', '').replace('rt', '') for word in mytokens ]\n",
        "      \n",
        "  mytokens = [ word for word in mytokens if word not in [\"\"] ]\n",
        "\n",
        "  return mytokens\n",
        "\n",
        "def preprocess(dataframe):\n",
        "\n",
        "  for index in dataframe.index:\n",
        "\n",
        "    # Get TEXT value\n",
        "    text_to_clean = str(dataframe.at[index,'text'])\n",
        "\n",
        "    # Remove hyperlink (http:/t.co.....)\n",
        "    text_to_clean = re.sub(r\"http\\S+\", \"\", text_to_clean)\n",
        "\n",
        "    text_to_clean = re.sub(r\"Â‰\\S+\", \"\", text_to_clean)\n",
        "\n",
        "    # Build cleaned TEXT\n",
        "    text_cleaned = ''.join(text_to_clean)\n",
        "\n",
        "    #--------------------------------------------------\n",
        "\n",
        "    # Cleaned TEXT into dataframe\n",
        "    dataframe.at[index,'text'] = text_cleaned\n",
        "\n",
        "    # Get 'KEYWORD' value\n",
        "    keyword_to_clean = str(dataframe.at[index,'keyword'])\n",
        "\n",
        "    # Remove '%20'\n",
        "    keyword_cleaned = keyword_to_clean.replace(\"%20\",\" \")\n",
        "\n",
        "    # Cleaned KEYWORD into dataframe\n",
        "    dataframe.at[index,'keyword'] = keyword_cleaned\n",
        "\n",
        "  return dataframe\n",
        "\n",
        "#df = df.drop(columns=['id'])\n",
        "\n",
        "df = preprocess(df)\n",
        "\n",
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3738</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>USA</td>\n",
              "      <td>Black Eye 9: A space battle occurred at Star O...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>853</td>\n",
              "      <td>bioterror</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#world FedEx no longer to transport bioterror ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10540</td>\n",
              "      <td>windstorm</td>\n",
              "      <td>Palm Beach County, FL</td>\n",
              "      <td>Reality Training: Train falls off elevated tra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5988</td>\n",
              "      <td>hazardous</td>\n",
              "      <td>USA</td>\n",
              "      <td>#Taiwan Grace: expect that large rocks trees m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6328</td>\n",
              "      <td>hostage</td>\n",
              "      <td>Australia</td>\n",
              "      <td>New ISIS Video: ISIS Threatens to Behead Croat...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6466</th>\n",
              "      <td>4377</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>ARGENTINA</td>\n",
              "      <td>#Earthquake #Sismo M 1.9 - 15km E of Anchorage...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6467</th>\n",
              "      <td>3408</td>\n",
              "      <td>derail</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@EmiiliexIrwin Totally agree.She is 23 and kno...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6468</th>\n",
              "      <td>9794</td>\n",
              "      <td>trapped</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Hollywood Movie About Trapped Miners Released ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6469</th>\n",
              "      <td>10344</td>\n",
              "      <td>weapons</td>\n",
              "      <td>Beirut/Toronto</td>\n",
              "      <td>Friendly reminder that the only country to eve...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6470</th>\n",
              "      <td>1779</td>\n",
              "      <td>buildings on fire</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Buildings are on fire and they have time for a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6471 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ... target\n",
              "0      3738  ...      0\n",
              "1       853  ...      0\n",
              "2     10540  ...      1\n",
              "3      5988  ...      1\n",
              "4      6328  ...      1\n",
              "...     ...  ...    ...\n",
              "6466   4377  ...      1\n",
              "6467   3408  ...      0\n",
              "6468   9794  ...      1\n",
              "6469  10344  ...      1\n",
              "6470   1779  ...      1\n",
              "\n",
              "[6471 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfhLGYPXRZZe"
      },
      "source": [
        "# 3: Train a model\n",
        "###Basic : \n",
        "- split train/test (80/20)\n",
        "- define TF-IDF vectorizer\n",
        "- define classifier\n",
        "- fit model on training set\n",
        "- first evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qwd6D4XRMYx"
      },
      "source": [
        "X = df['text']\n",
        "y = df['target']\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=59)\n",
        "\n",
        "X_train\n",
        "\n",
        "# Define vectorizer\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), tokenizer=spacy_token)\n",
        "\n",
        "# Define classifier\n",
        "classifier = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=59)\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf),\n",
        "                ('classifier', classifier)])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDQVP6RVcaZO",
        "outputId": "93852294-085a-4233-a76c-67818f5e6751"
      },
      "source": [
        "# Fit model on training set\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "nFeat = 0\n",
        "for i in tfidf.get_feature_names():\n",
        "  nFeat += 1\n",
        "print(\"number of features created by the TF-IDF vector :\",str(nFeat))\n",
        "\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "#first evaluation\n",
        "print(f\"ACCURACY SCORE test:\\n{accuracy_score(y_test, y_pred):.3f}\")\n",
        "\n",
        "print(f\"ACCURACY SCORE train:\\n{accuracy_score(y_train, pipe.predict(X_train)):.3f}\")\n",
        "\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of features created by the TF-IDF vector : 2286\n",
            "ACCURACY SCORE test:\n",
            "0.788\n",
            "ACCURACY SCORE train:\n",
            "0.859\n",
            "CONFUSION MATRIX:\n",
            "[[644  99]\n",
            " [175 377]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcPYVvwvVOEJ"
      },
      "source": [
        "# 4: First submission\n",
        "###Basic : \n",
        "- mount the drive in order to save the csv with submission values\n",
        "- split train/test (80/20)\n",
        "- define TF-IDF vectorizer\n",
        "- define classifier\n",
        "- fit model on training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXtrrjWUUGIA",
        "outputId": "109683de-fd00-4919-a220-39511b444866"
      },
      "source": [
        "#Mount a google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('drive')\n",
        "\n",
        "\n",
        "#To improve performance : drop unused columns ('id' for now)\n",
        "test_data = df_test['text']\n",
        "\n",
        "\n",
        "# Get & print the number of features generated by the TF-IDF vector\n",
        "nFeat = 0\n",
        "for i in tfidf.get_feature_names():\n",
        "  nFeat += 1\n",
        "print(\"number of features created by the TF-IDF vector :\",str(nFeat))\n",
        "\n",
        "test_pred = pipe.predict(test_data)\n",
        "print(type(test_pred))\n",
        "print(test_pred)\n",
        "\n",
        "#first evaluation\n",
        "print(f\"ACCURACY SCORE test:\\n{accuracy_score(y_test, y_pred):.3f}\")\n",
        "\n",
        "print(f\"ACCURACY SCORE train:\\n{accuracy_score(y_train, pipe.predict(X_train)):.3f}\")\n",
        "\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at drive\n",
            "number of features created by the TF-IDF vector : 2286\n",
            "<class 'numpy.ndarray'>\n",
            "[0 0 1 ... 1 0 1]\n",
            "ACCURACY SCORE test:\n",
            "0.788\n",
            "ACCURACY SCORE train:\n",
            "0.859\n",
            "CONFUSION MATRIX:\n",
            "[[644  99]\n",
            " [175 377]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgCtCklMbyMt"
      },
      "source": [
        "#Handle the submission file\n",
        "submission_file = pd.DataFrame(test_pred,columns=['target'])\n",
        "\n",
        "submission_file.to_csv('submission_one.csv', index=False)\n",
        "\n",
        "!cp submission_one.csv \"drive/My Drive/Team_Microsoft\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}